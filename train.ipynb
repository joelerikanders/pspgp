{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jaP6UJOjisnx"},"outputs":[],"source":["import warnings\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import pickle\n","import gc\n","import os.path\n","import copy\n","import shutil\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.tensorboard import SummaryWriter\n","from xgboost import XGBClassifier, XGBRegressor\n","from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n","from sklearn.metrics import f1_score\n","from sklearn.linear_model import LinearRegression\n","from scipy.optimize import minimize\n","\n","!pip install catboost\n","from catboost import CatBoostClassifier, Pool\n","\n","!pip install polars==0.18.2\n","import polars as pl\n","\n","!pip install transformers\n","from transformers import get_linear_schedule_with_warmup\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Swf-HNN_ltvB"},"outputs":[],"source":["%cd drive/My\\ Drive/performance_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BS_Pm9H9rxoP"},"outputs":[],"source":["np.random.seed(0)\n","\n","def load_data(n_rows=None):\n","    \"\"\"\n","    Load n_rows rows of the kaggle dataset and the parsed raw data,\n","    cast to specified data types,\n","    and fill nulls\n","\n","    Returns:\n","    df: DataFrame\n","    concatenation of the kaggle dataset and parsed raw data\n","\n","    orig_ids: Series\n","    The session ids of the kaggle dataset\n","    \"\"\"\n","\n","    dtypes = {\n","        \"session_id\": pl.Int64,\n","        \"elapsed_time\": pl.Int64,\n","        \"level\": pl.Int8,\n","        \"page\": pl.Float32,\n","        \"room_coor_x\": pl.Float32,\n","        \"room_coor_y\": pl.Float32,\n","        \"screen_coor_x\": pl.Float32,\n","        \"screen_coor_y\": pl.Float32,\n","        \"hover_duration\": pl.Float32,\n","        \"fullscreen\": pl.Int8,\n","        \"hq\": pl.Int8,\n","        \"music\": pl.Int8,\n","    }\n","    dtypes = [pl.col(key).cast(value) for key, value in dtypes.items()]\n","    fills = [\n","        pl.col(\"page\").fill_null(-1),\n","        pl.col(\"fqid\").fill_null(\"fqid_None\"),\n","        pl.col(\"text_fqid\").fill_null(\"text_fqid_None\")\n","    ]\n","    to_cat = [\n","        pl.col(\"fqid\").cast(pl.Categorical),\n","        pl.col(\"text_fqid\").cast(pl.Categorical),\n","        pl.col(\"room_fqid\").cast(pl.Categorical),\n","        pl.col(\"text\").cast(pl.Categorical),\n","        pl.col(\"event_name\").cast(pl.Categorical),\n","        pl.col(\"name\").cast(pl.Categorical),\n","    ]\n","\n","    df = (pl.read_parquet(\"data/train.parquet\", n_rows=n_rows)\n","            .with_columns(dtypes)\n","            .with_columns(fills))\n","\n","    df_raw = (pl.read_parquet(\"data/raw_train.parquet\", n_rows=n_rows)\n","                .filter(~pl.col(\"session_id\").is_in(df[\"session_id\"].unique()))\n","                .with_columns(dtypes)\n","                .with_columns(fills))\n","\n","    orig_ids = df[\"session_id\"].unique(maintain_order=True)\n","\n","    df = pl.concat([df, df_raw])\n","\n","    # reduce memory usage\n","    df = df.with_columns(to_cat)\n","\n","    return df, orig_ids\n","\n","\n","\n","def preprocess(df):\n","    \"\"\"\n","    Preprocess the data\n","    \"\"\"\n","\n","    # sort by level group and index\n","    grp_map = {\"0-4\": 1, \"5-12\": 2, \"13-22\": 3}\n","    df = df.with_columns(pl.col(\"level_group\").apply(lambda x: grp_map[x]))\n","    df = df.sort(by=[\"session_id\", \"level_group\", \"index\"])\n","    grp_map = {1: \"0-4\", 2: \"5-12\", 3: \"13-22\"}\n","    df = df.with_columns(pl.col(\"level_group\").apply(lambda x: grp_map[x]))\n","\n","    # drop hovers and create new index for each session, from 0 to len(session)\n","    df = df.filter(pl.col(\"hover_duration\").is_null())\n","    df = df.with_columns(pl.arange(0, pl.col(\"index\").count()).over([\"session_id\"]).alias(\"new_index\"))\n","    df = df.drop(\"index\").rename({\"new_index\": \"index\"})\n","\n","    # remove old sessions\n","    df = df.with_columns([\n","        pl.col(\"session_id\").apply(lambda x: int(str(x)[:4])).alias(\"year_month\"),\n","    ])\n","    df = df.filter(pl.col(\"year_month\") > 1911)\n","\n","    return df\n","\n","\n","def split_data(df):\n","    \"\"\"\n","    Splits the data for training models per level group\n","\n","    half_group_ids: Series\n","    session ids that contains all levels 0-12 but not all 13-22\n","    \"\"\"\n","\n","    df1 = df.filter((pl.col(\"level_group\")=='0-4'))\n","    df1 = df1.filter((pl.col(\"level\").n_unique() == 5).over(\"session_id\"))\n","    df2 = df.filter((pl.col(\"level_group\").is_in([\"0-4\", '5-12'])))\n","    df2 = df2.filter((pl.col(\"level\").n_unique() == 13).over(\"session_id\"))\n","    df3 = df.filter((pl.col(\"level\").n_unique() == 23).over(\"session_id\"))\n","\n","    half_group_ids = df2.filter(~pl.col(\"session_id\").is_in(df3[\"session_id\"].unique()))[\"session_id\"].unique()\n","\n","    return df1, df2, df3, half_group_ids\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tb1a1t3-Gb7a"},"outputs":[],"source":["df, orig_ids = load_data(n_rows=None)\n","df = preprocess(df)\n","df1, df2, df3, half_group_ids = split_data(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3Wk48r1G7Eu"},"outputs":[],"source":["del df\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQEhSl-5sbVz"},"outputs":[],"source":["df1[\"session_id\"].n_unique(), df2[\"session_id\"].n_unique(), df3[\"session_id\"].n_unique(), half_group_ids.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKcdV2J_upyd"},"outputs":[],"source":["CATS = [\n","    'event_name',\n","    'name',\n","    'fqid',\n","    'room_fqid',\n","    'text_fqid'\n","]\n","\n","NUMS = [\n","    'room_coor_x',\n","    'room_coor_y',\n","    'screen_coor_x',\n","    'screen_coor_y',\n","    'et_diff',\n","    'page'\n","]\n","\n","SAVE_FOR_SUB = True\n","# To train a simplified model, set ONLY_TR_FEATURES=True,\n","# and run only the XGBoost training cell out of the models.\n","ONLY_TR_FEATURES = False\n","\n","# for only using categoricals present in respective level group df,\n","# when feature engineering\n","grp_cat_vals = {}\n","for cat in CATS:\n","    for grp, df in [\n","        (\"0-4\", df1),\n","        (\"5-12\", df2),\n","        (\"13-22\", df3),\n","    ]:\n","        grp_cat_vals[(grp, cat)] = df[cat].unique(maintain_order=True)\n","\n","df_temp = df3.with_columns([\n","    pl.col(\"page\").cast(pl.Utf8),\n","    pl.col(\"level\").cast(pl.Utf8),\n","])\n","# identify points in the game with \"point_columns\"\n","point_columns = [\"session_id\", \"event_name\", \"level\", \"name\", \"page\", \"fqid\", \"room_fqid\", \"text_fqid\"]\n","# enumerate duplicate points\n","df_temp = df_temp.with_columns(pl.col(\"index\").cumcount().over(point_columns).cast(pl.Utf8).alias(\"dup_count\"))\n","df_temp = df_temp.with_columns((pl.col(\"event_name\") + pl.col(\"level\") + pl.col(\"name\") + pl.col(\"page\") + pl.col(\"fqid\") + pl.col(\"room_fqid\") + pl.col(\"text_fqid\") + pl.col(\"dup_count\")).alias(\"point\"))\n","# identify points that are present in over 0.999 of sessions\n","most_common = df_temp[\"point\"].value_counts(sort=True).filter(pl.col(\"counts\") >= 0.999*df_temp[\"session_id\"].n_unique())[\"point\"]\n","df_temp = df_temp.filter(pl.col(\"point\").is_in(most_common))\n","\n","# the order of occurance of points in the game\n","point_pos = df_temp.groupby([\"point\", \"level_group\"]).agg(pl.col(\"index\").mean()).sort(by=\"index\")\n","# a map from points to integer\n","point_ixs = {cat: i for i, cat in enumerate(most_common, 1)}\n","\n","del df_temp\n","_ = gc.collect()\n","\n","# save for feature engineering during inference and/or training\n","if SAVE_FOR_SUB:\n","    f_save = open(\"models/xgb_models/grp_cat_vals.pkl\", 'wb')\n","    pickle.dump(grp_cat_vals, f_save)\n","    f_save.close()\n","\n","    f_save = open('models/torch_models/point_ixs.pkl', 'wb')\n","    pickle.dump(point_ixs, f_save)\n","    f_save.close()\n","\n","    pl.DataFrame(most_common).write_parquet(\"models/xgb_models/most_common.parquet\")\n","\n","    pl.DataFrame(most_common).write_parquet(\"models/torch_models/most_common.parquet\")\n","\n","    point_pos.write_parquet(\"models/xgb_models/point_pos.parquet\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYRoEmS9Nnbg"},"outputs":[],"source":["def feature_engineer_base(df):\n","    \"\"\"\n","    Create initial features, distance (cumulative distance moved) and et_diff,\n","    that are used to create other features\n","    in feature_engineer_xgb and feature_engineer_tr\n","    \"\"\"\n","    cols = [\n","        (\n","            (((pl.col(\"screen_coor_y\")-pl.col(\"screen_coor_y\").shift(1)).fill_null(0) ** 2\n","            + (pl.col(\"screen_coor_x\")-pl.col(\"screen_coor_x\").shift(1)).fill_null(0) ** 2) ** (1/2))\n","            .cumsum()\n","            .fill_null(0)\n","            .over([\"session_id\"])\n","            .alias(\"distance\")\n","        ),\n","        (\n","            (pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1))\n","            .fill_null(0)\n","            .clip(0, 1e9)\n","            .over([\"session_id\"])\n","            .alias(\"et_diff\")\n","        ),\n","    ]\n","    return df.with_columns(cols)\n","\n","\n","def feature_engineer_xgb(df, grp):\n","    \"\"\"\n","    Create features that only xgboost and catboost uses\n","    \"\"\"\n","\n","    aggs = [\n","\n","        # fqid\n","        # count of each fqid value\n","        *[((pl.col(\"fqid\") == c).sum()).alias(f\"{c}_num\") for c in grp_cat_vals[(grp, \"fqid\")]],\n","        # mean elapsed time difference over each fqid value\n","        *[pl.col(\"et_diff\").filter(pl.col(\"fqid\")==c).mean().alias(f\"{c}_et_mean\") for c in grp_cat_vals[(grp, \"fqid\")]],\n","        # max elapsed time difference over each fqid value\n","        *[pl.col(\"et_diff\").filter(pl.col(\"fqid\")==c).max().alias(f\"{c}_et_max\") for c in grp_cat_vals[(grp, \"fqid\")]],\n","\n","        # room_fqid\n","        *[((pl.col(\"room_fqid\") == c).sum()).alias(f\"{c}_num\") for c in grp_cat_vals[(grp, \"room_fqid\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"room_fqid\")==c).mean().alias(f\"{c}_et_mean\") for c in grp_cat_vals[(grp, \"room_fqid\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"room_fqid\")==c).max().alias(f\"{c}_et_max\") for c in grp_cat_vals[(grp, \"room_fqid\")]],\n","\n","        # text_fqid\n","        *[((pl.col(\"text_fqid\") == c).sum()).alias(f\"{c}_num\") for c in grp_cat_vals[(grp, \"text_fqid\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"text_fqid\")==c).mean().alias(f\"{c}_et_mean\") for c in grp_cat_vals[(grp, \"text_fqid\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"text_fqid\")==c).max().alias(f\"{c}_et_max\") for c in grp_cat_vals[(grp, \"text_fqid\")]],\n","\n","        # event_name\n","        *[((pl.col(\"event_name\") == c).sum()).alias(f\"{c}_num\") for c in grp_cat_vals[(grp, \"event_name\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"event_name\")==c).mean().alias(f\"{c}_et_mean\") for c in grp_cat_vals[(grp, \"event_name\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"event_name\")==c).max().alias(f\"{c}_et_max\") for c in grp_cat_vals[(grp, \"event_name\")]],\n","\n","        # name\n","        *[((pl.col(\"name\") == c).sum()).alias(f\"{c}_num\") for c in grp_cat_vals[(grp, \"name\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"name\")==c).mean().alias(f\"{c}_et_mean\") for c in grp_cat_vals[(grp, \"name\")]],\n","        *[pl.col(\"et_diff\").filter(pl.col(\"name\")==c).max().alias(f\"{c}_et_max\") for c in grp_cat_vals[(grp, \"name\")]],\n","\n","        # stats of the numericals\n","        *[pl.col(c).drop_nulls().n_unique().alias(f\"{c}_unique\") for c in CATS],\n","        *[pl.col(c).mean().alias(f\"{c}_mean\") for c in NUMS],\n","        *[pl.col(c).max().alias(f\"{c}_max\") for c in NUMS],\n","        *[pl.col(c).min().alias(f\"{c}_min\") for c in NUMS],\n","        *[pl.col(c).std().alias(f\"{c}_std\") for c in NUMS],\n","        *[pl.col(c).sum().alias(f\"{c}_sum\") for c in NUMS],\n","\n","        # whether or not hq, music, fullscreen are used\n","        pl.col(\"hq\").apply(lambda x: x[0]),\n","        pl.col(\"music\").apply(lambda x: x[0]),\n","        pl.col(\"fullscreen\").apply(lambda x: x[0]),\n","\n","    ]\n","\n","    df = df.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n","\n","    # weekday and hour\n","    df = df.with_columns([\n","        pl.col(\"session_id\").apply(lambda x: int(str(x)[4:6])).alias(\"weekday\"),\n","        pl.col(\"session_id\").apply(lambda x: int(str(x)[6:8])).alias(\"hour\"),\n","    ])\n","\n","    return df\n","\n","\n","def point_filter_tr(df):\n","    \"\"\"\n","    Filter out the points that exists in more than 0.999 of sessions\n","    (that are in most_common)\n","    \"\"\"\n","    df = df.with_columns(pl.col(\"index\").cumcount().over([\"session_id\", \"event_name\", \"level\", \"name\", \"page\", \"fqid\", \"room_fqid\", \"text_fqid\"]).cast(pl.Utf8).alias(\"dup_count\"))\n","    df = df.with_columns((pl.col(\"event_name\") + pl.col(\"level\") + pl.col(\"name\") + pl.col(\"page\") + pl.col(\"fqid\") + pl.col(\"room_fqid\") + pl.col(\"text_fqid\") + pl.col(\"dup_count\")).alias(\"point\"))\n","    df = df.filter(pl.col(\"point\").is_in(most_common))\n","    return df\n","\n","\n","def pad_tr(df, grp):\n","    \"\"\"\n","    Makes sure that the transformer features are of constant length\n","    and in the same order when used by the gbdt models.\n","    \"\"\"\n","    # the level group predicted mapped to the level groups seen/used\n","    point_lgs = {\"0-4\": [\"0-4\"], \"5-12\": [\"0-4\", \"5-12\"], \"13-22\": [\"0-4\", \"5-12\", \"13-22\"]}[grp]\n","    # get order of occurance of the points used\n","    point_pos_lgs = point_pos.filter(pl.col(\"level_group\").is_in(point_lgs))\n","    seq_len = min(256, point_pos_lgs.shape[0])\n","    order = point_pos_lgs[-seq_len:]\n","\n","    def sort_points(df_sess):\n","        \"\"\"\n","        sorts the points in a df containing one session,\n","        in order specified by order\n","        \"\"\"\n","        sid = df_sess[\"session_id\"][0]\n","        p = order.with_columns(pl.lit(sid).alias(\"session_id\"))\n","        return p.join(df_sess, on=\"point\", how=\"left\")\n","\n","    df = df.groupby(\"session_id\", maintain_order=True).apply(sort_points)\n","    df = df.fill_null(0)\n","    return df\n","\n","\n","def feature_engineer_tr(df):\n","    \"\"\"\n","    Generate the numerical transformer features:\n","    time difference,\n","    index difference,\n","    distance (cumulative distance moved, calculated from screen_coor's) difference,\n","    room_coor_x,\n","    room_coor_y\n","    \"\"\"\n","    return df.with_columns([\n","        (pl.col(\"elapsed_time\").clip(0, 1e9) - pl.col(\"elapsed_time\").clip(0, 1e9).shift(1)).fill_null(0).over(\"session_id\").alias(\"et_diff\"),\n","        (pl.col(\"index\") - pl.col(\"index\").shift(1)).fill_null(0).over(\"session_id\").alias(\"ix_diff\"),\n","        (pl.col(\"distance\") - pl.col(\"distance\").shift(1)).fill_null(0).over(\"session_id\").alias(\"dist_diff\"),\n","        pl.col(\"room_coor_x\").fill_null(0),\n","        pl.col(\"room_coor_y\").fill_null(0),\n","    ])\n","\n","\n","def flatten_tr(df):\n","    \"\"\"\n","    Flatten the transformer features so they can be used by the GBDT models\n","    \"\"\"\n","    df = df.select([\"session_id\", \"et_diff\", \"ix_diff\", \"dist_diff\", \"room_coor_x\", \"room_coor_y\"])\n","    df = df.groupby(\"session_id\").agg([pl.col(c) for c in df.columns[1:]])\n","    vs = {}\n","    for r in df.iter_rows():\n","        ls = []\n","        for l in r[1:]:\n","            ls.extend(l)\n","        vs[str(r[0])] = ls\n","    df = pl.DataFrame(vs).transpose(include_header=True)\n","    df = df.with_columns(pl.col(\"column\").cast(pl.Int64)).rename({\"column\": \"session_id\"})\n","    return df\n","\n","\n","def xgb_data_pipe(df, grp):\n","    \"\"\"\n","    Generate all features for training and infering with the GBDT models\n","    \"\"\"\n","    df = feature_engineer_base(df)\n","    # convert to string for combining the categoricals when creating points\n","    df_tr = df.with_columns([\n","        pl.col(\"page\").cast(pl.Utf8),\n","        pl.col(\"level\").cast(pl.Utf8),\n","    ])\n","    df_tr = point_filter_tr(df_tr)\n","    df_tr = feature_engineer_tr(df_tr)\n","    df_tr = pad_tr(df_tr, grp)\n","    df_tr = flatten_tr(df_tr)\n","    if ONLY_TR_FEATURES:\n","        return df_tr\n","    else:\n","        df_xgb = feature_engineer_xgb(df, grp)\n","        df = df_xgb.join(df_tr, on=\"session_id\", how=\"inner\")\n","        return df\n","\n","\n","def xgb_fs(df):\n","    \"\"\"\n","    Some feature selection for the GBDT models\n","    \"\"\"\n","    corr = df.corr().with_columns(pl.col(\"*\").abs())\n","    # put features correlated over 0.99 in the \"drop\" list to be dropped\n","    drop = [col for i, col in enumerate(corr.columns) if any(corr[col][:i] > 0.99)]\n","    df = df.to_pandas().set_index('session_id')\n","    # number of nulls\n","    null = df.isnull().sum().sort_values(ascending=False) / len(df)\n","    # drop features with more than 0.75 nulls\n","    drop += list(null[null>0.75].index)\n","    for col in df.columns:\n","        if df[col].nunique() == 1:\n","            # drop features with only one unique value\n","            drop.append(col)\n","    features = np.array([f for f in df.columns if f not in drop + [\"level_group\"]])\n","    print(len(drop), features.shape)\n","    return df, features\n","\n","\n","def tr_data_pipe(df):\n","    \"\"\"\n","    Generate all features for training and infering with the transformer\n","    \"\"\"\n","    df = feature_engineer_base(df)\n","    # convert to string for combining the categoricals when creating points\n","    df = df.with_columns([\n","        pl.col(\"page\").cast(pl.Utf8),\n","        pl.col(\"level\").cast(pl.Utf8),\n","    ])\n","    df = point_filter_tr(df)\n","    df = feature_engineer_tr(df)\n","    df = df.with_columns([\n","        np.sign(pl.col(f)) * np.log1p(np.absolute(pl.col(f))) for f in [\"et_diff\", \"ix_diff\", \"dist_diff\"]\n","    ])\n","    return df.select([\"session_id\", \"level_group\", \"et_diff\", \"ix_diff\",  \"dist_diff\", \"room_coor_x\", \"room_coor_y\", \"point\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqjhpxCZhvj8"},"outputs":[],"source":["%%time\n","df1_xgb = xgb_data_pipe(df1, \"0-4\")\n","df2_xgb = xgb_data_pipe(df2, \"5-12\")\n","df3_xgb = xgb_data_pipe(df3, \"13-22\")\n","\n","df1_xgb, features1 = xgb_fs(df1_xgb)\n","df2_xgb, features2 = xgb_fs(df2_xgb)\n","df3_xgb, features3 = xgb_fs(df3_xgb)\n","\n","if SAVE_FOR_SUB:\n","    feature_dict = {\n","        \"0-4\": features1,\n","        \"5-12\": features2,\n","        \"13-22\": features3\n","    }\n","    f_name = \"feature_dict_tr.pkl\" if ONLY_TR_FEATURES else \"feature_dict.pkl\"\n","    f_save = open('models/xgb_models/' + f_name, 'wb')\n","    pickle.dump(feature_dict, f_save)\n","    f_save.close()\n","\n","df3_xgb.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcpexSnYpmzq"},"outputs":[],"source":["%%time\n","# create the transformer input\n","df_tr = tr_data_pipe(df3)\n","# use the incomplete sessions for the transformer aswell\n","df_tr_h = tr_data_pipe(df2.filter(pl.col(\"session_id\").is_in(half_group_ids)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2krZg67LbkAQ"},"outputs":[],"source":["del df1, df2, df3\n","_ = gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_rR6E6Nijci"},"outputs":[],"source":["seq_lens = []\n","seq_len = 0\n","for g in [\"0-4\", \"5-12\", \"13-22\"]:\n","    seq_len += point_pos.filter(pl.col(\"level_group\") == g).shape[0]\n","    print(f\"seq_len {g}:\", seq_len)\n","    seq_lens.append(seq_len)\n","max_seq_len = 256\n","trimmed_seq_lens = [min(p, max_seq_len) for p in seq_lens]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjNCin3sq1t3"},"outputs":[],"source":["targets = pd.read_csv(\"data/train_labels.csv\")\n","targets['session'] = targets.session_id.apply(lambda x: int(x.split('_')[0]))\n","targets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-8q_g0BrL5z"},"outputs":[],"source":["# load targets from the kaggle data and raw data, and put them in the same format\n","\n","targets_raw = pl.read_parquet(\"data/raw_labels.parquet\")\n","targets_raw = targets_raw.filter(~pl.col(\"session_id\").is_in(targets[\"session\"].unique()))\n","\n","sids = []\n","correct = []\n","session = []\n","q = []\n","for i in range(18):\n","    for sid in targets_raw[\"session_id\"]:\n","        sids.append(str(sid) + f\"_q{i+1}\")\n","        session.append(sid)\n","        q.append(i+1)\n","\n","for i in range(18):\n","    correct.extend(targets_raw[f\"q{i}\"].to_list())\n","\n","targets_raw = pd.DataFrame({\n","    \"session_id\": sids,\n","    \"correct\": correct,\n","    \"session\": session,\n","    \"q\": q\n","})\n","\n","targets_xgb = pd.concat([targets, targets_raw])\n","targets_xgb.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdQa1Z6xrOIS"},"outputs":[],"source":["sids = {\"session_id\": [k for k in targets_xgb.session.unique()]}\n","qs = {f\"q{i}\": targets_xgb[targets_xgb.q == i+1].correct.values  for i in range(18)}\n","targets_tr = pl.DataFrame(sids | qs)\n","targets_tr.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBMcNLLH71bB"},"outputs":[],"source":["# use only the kaggle data for validation, specified in val_sids\n","val_sids = df_tr.filter(pl.col(\"session_id\").is_in(orig_ids))[\"session_id\"].unique(maintain_order=True)\n","qs = {f\"q{i+1}\": [0]*val_sids.shape[0] for i in range(18)}\n","xgb_oof = pd.DataFrame({\"session_id\": val_sids} | qs).set_index(\"session_id\")\n","tr_oof = pd.DataFrame({\"session_id\": val_sids} | qs).set_index(\"session_id\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1E7BFRlw49xI"},"outputs":[],"source":["def threshold_score(preds, targs):\n","    \"\"\"\n","    Calculate the best threshold by simply iterating and taking the maximum\n","    \"\"\"\n","    scores = []\n","    thresholds = np.arange(0.4, 0.81, 0.005)\n","    for threshold in thresholds:\n","        preds_bin = (preds > threshold).astype('int')\n","        score = f1_score(targs, preds_bin, average=\"macro\")\n","        scores.append(score)\n","    best_score = max(scores)\n","    best_threshold = thresholds[scores.index(best_score)]\n","    return best_score, best_threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gPZXGZu49zi"},"outputs":[],"source":["%%time\n","\n","# XGB training\n","\n","n_splits = 5\n","\n","kf = KFold(n_splits=n_splits)\n","\n","seeds = [0] if ONLY_TR_FEATURES else [0, 1, 2]\n","for s in seeds:\n","\n","    params = {\n","        'objective' : 'binary:logistic',\n","        'eval_metric':'logloss',\n","        'learning_rate': 0.015,\n","        'max_depth': 8,\n","        'n_estimators': 1000,\n","        'early_stopping_rounds': 50,\n","        'tree_method':'gpu_hist',\n","        'subsample': 0.8,\n","        'colsample_bytree': 0.4,\n","        'use_label_encoder' : False,\n","        \"alpha\": 8,\n","        \"lambda\": 0,\n","        \"seed\": s\n","    }\n","\n","    limits = {'0-4': range(1, 4), '5-12': range(4, 14), '13-22': range(14, 19)}\n","\n","    fold_preds = []\n","    fold_targs = []\n","\n","    for i, (train_index, test_index) in enumerate(kf.split(val_sids)):\n","\n","        grp_preds = []\n","        grp_targs = []\n","\n","        for df, features, grp in [\n","            (df1_xgb, features1, \"0-4\"),\n","            (df2_xgb, features2, \"5-12\"),\n","            (df3_xgb, features3, \"13-22\")\n","        ]:\n","\n","            test_users = val_sids[test_index]\n","            test_x = df[features].loc[test_users]\n","\n","            # Doing this to validate on the kaggle data but also use the raw data for training\n","            train_x = df[features].loc[~df.index.isin(test_users)]\n","            train_users = train_x.index.values\n","\n","            train_y = targets_xgb.loc[targets_xgb[\"q\"].isin(limits[grp])].set_index('session').loc[train_users]\n","            train_y = train_y.sort_values(by=[\"session\", \"q\"])\n","            # create copies of the data for each question\n","            train_x = pd.concat([train_x]*len(limits[grp])).sort_values(by=[\"session_id\"])\n","            # use question number as a input feature since one model is trained per level group\n","            train_x[\"q\"] = train_y[\"q\"].values\n","            train_y = train_y['correct'].values\n","\n","            test_y = targets_xgb.loc[targets_xgb[\"q\"].isin(limits[grp])].set_index('session').loc[test_users]\n","            test_y = test_y.sort_values(by=[\"session\", \"q\"])\n","            test_x = pd.concat([test_x]*len(limits[grp])).sort_values(by=[\"session_id\"])\n","            test_x[\"q\"] = test_y[\"q\"].values\n","            test_y = test_y['correct'].values\n","\n","            clf = XGBClassifier(**params)\n","\n","\n","            if ONLY_TR_FEATURES:\n","                model_path = f\"models/xgb_models/model_tr_{i}_grp_{grp}_s{params['seed']}.xgb\"\n","            else:\n","                model_path = f\"models/xgb_models/model_{i}_grp_{grp}_s{params['seed']}.xgb\"\n","\n","            if os.path.exists(model_path):\n","                clf.load_model(model_path)\n","            else:\n","                clf.fit(train_x.astype(np.float32), train_y, eval_set=[(test_x.astype(np.float32), test_y)], verbose=0)\n","\n","            val_probas = clf.predict_proba(test_x.astype(np.float32))[:, 1]\n","\n","            grp_preds.append(val_probas)\n","            grp_targs.append(test_y)\n","\n","            if SAVE_FOR_SUB:\n","                clf.save_model(model_path)\n","\n","            for j, q in enumerate(limits[grp]):\n","                xgb_oof.loc[test_x.index.unique(), f\"q{q}\"] = val_probas[j::len(limits[grp])]\n","\n","        preds = np.concatenate(grp_preds)\n","        targs = np.concatenate(grp_targs)\n","\n","        best_score, best_threshold = threshold_score(preds, targs)\n","\n","        fold_preds.append(preds)\n","        fold_targs.append(targs)\n","\n","        print(f\"fold_score_{i}:\", best_score)\n","\n","    preds = np.concatenate(fold_preds)\n","    targs = np.concatenate(fold_targs)\n","\n","    best_score, best_threshold = threshold_score(preds, targs)\n","\n","    if ONLY_TR_FEATURES:\n","        xgb_oof.to_csv(f\"models/xgb_models/xgb_oof_tr_s{params['seed']}.csv\")\n","    else:\n","        xgb_oof.to_csv(f\"models/xgb_models/xgb_oof_s{params['seed']}.csv\")\n","\n","    print(\"-\"*10)\n","    print(\"cv_score:\", best_score)\n","    print(\"best_threshold:\", best_threshold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C35uiygOf4PO"},"outputs":[],"source":["# evaluate xgb seeds average\n","\n","targs = targets_xgb.set_index('session').loc[val_sids][\"correct\"].values\n","\n","seeds_preds = 0\n","for s in seeds:\n","    preds = pd.read_csv(f\"models/xgb_models/xgb_oof_s{s}.csv\").set_index(\"session_id\")\n","    seeds_preds += preds.values.flatten() / len(seeds)\n","\n","threshold_score(seeds_preds, targs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICPISLVVCadv"},"outputs":[],"source":["if SAVE_FOR_SUB:\n","    # Used for normalizing the transformer input during training and inference\n","    # The input is normalized over each point\n","    norms = df_tr.filter(pl.col(\"session_id\").is_in(orig_ids)).groupby(\"point\").agg([\n","        pl.col(\"et_diff\").mean().alias(\"et_mean\"),\n","        pl.col(\"ix_diff\").mean().alias(\"ix_mean\"),\n","        pl.col(\"dist_diff\").mean().alias(\"dist_mean\"),\n","        pl.col(\"et_diff\").std().alias(\"et_std\"),\n","        pl.col(\"ix_diff\").std().alias(\"ix_std\"),\n","        pl.col(\"dist_diff\").std().alias(\"dist_std\"),\n","        pl.col(\"room_coor_x\").mean().alias(\"room_coor_x_mean\"),\n","        pl.col(\"room_coor_y\").mean().alias(\"room_coor_y_mean\"),\n","        pl.col(\"room_coor_x\").std().alias(\"room_coor_x_std\"),\n","        pl.col(\"room_coor_y\").std().alias(\"room_coor_y_std\"),\n","    ])\n","    norms.write_parquet(f\"models/torch_models/norms.parquet\")\n","\n","df_tr = pl.concat([df_tr, df_tr_h])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9mQvaGr3tMtG"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir=runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDEzIpZHbIzd"},"outputs":[],"source":["class NN(nn.Module):\n","    \"\"\"\n","    Lightweight transformer model\n","    \"\"\"\n","\n","    def __init__(self, num_cont_cols, embed_dim, num_layers, num_heads, max_seq_len):\n","        super(NN, self).__init__()\n","        self.emb_cont = nn.Sequential(\n","            nn.Linear(num_cont_cols, embed_dim//2),\n","            nn.LayerNorm(embed_dim//2)\n","        )\n","        self.emb_cats = nn.Sequential(\n","            nn.Embedding(max_seq_len + 1, embed_dim//2),\n","            nn.LayerNorm(embed_dim//2)\n","        )\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embed_dim,\n","            nhead=num_heads,\n","            dim_feedforward=embed_dim,\n","            dropout=0.1,\n","            batch_first=True,\n","            activation=\"relu\",\n","        )\n","        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","        # one head for each level group\n","        self.clf_heads = nn.ModuleList([\n","            nn.Linear(embed_dim, out_dim) for out_dim in [3, 10, 5]\n","        ])\n","\n","    def forward(self, x, grp):\n","        emb_conts = self.emb_cont(x[:, :, :-1])\n","        emb_cats = self.emb_cats(x[:, :, -1].type(torch.int32))\n","        x = torch.cat([emb_conts, emb_cats], dim=2)\n","        x = self.encoder(x)\n","        x = x.mean(dim=1)\n","        x = self.clf_heads[[\"0-4\", \"5-12\", \"13-22\"].index(grp)](x)\n","        return x.unsqueeze(2)\n","\n","\n","class DataGenerator(Dataset):\n","    \"\"\"\n","    The transformers datagenerator\n","    \"\"\"\n","    def __init__(self, xs, ys, seq_lens):\n","        self.xs = xs\n","        self.ys = ys\n","        self.pad_lens = seq_lens\n","\n","    def __len__(self):\n","        return len(self.xs)\n","\n","    def transform(self, x):\n","        padded_seqs = []\n","\n","        seqs = [x[0], np.concatenate([x[0], x[1]])]\n","        # since some of the raw data used does not have levelgroup 13-22\n","        if len(x) == 3:\n","            seqs.append(np.concatenate([x[0], x[1], x[2]]))\n","\n","        # padding and/or cropping\n","        for seq, pad_len in zip(seqs, self.pad_lens):\n","            seq_len = seq.shape[0]\n","            if seq_len < pad_len:\n","                seq = np.pad(seq, ((pad_len - seq_len, 0), (0, 0)), 'constant')\n","            if seq_len > pad_len:\n","                seq = seq[-pad_len:]\n","            padded_seqs.append(seq)\n","\n","        if len(padded_seqs) == 2:\n","            padded_seqs.append(np.zeros((self.pad_lens[-1], x[0].shape[1])))\n","\n","        return padded_seqs\n","\n","    def __getitem__(self, idx):\n","        x = self.xs[idx]\n","        y = self.ys[idx]\n","        x1, x2, x3 = self.transform(x)\n","        return x1, x2, x3, y\n","\n","\n","def train(hyperparams, train_x, train_y, test_x, test_y, seq_lens):\n","    \"\"\"\n","    Pytorch training loop\n","    \"\"\"\n","\n","    torch.manual_seed(hyperparams[\"seed\"])\n","    writer = SummaryWriter()\n","\n","    train_gen = DataGenerator(train_x, train_y, seq_lens)\n","    train_loader = DataLoader(\n","        train_gen,\n","        batch_size=hyperparams[\"batch_size\"],\n","        shuffle=True,\n","        num_workers=4,\n","        drop_last=True\n","    )\n","\n","    val_gen = DataGenerator(test_x, test_y, seq_lens)\n","    val_loader = DataLoader(\n","        val_gen,\n","        batch_size=hyperparams[\"batch_size\"],\n","        shuffle=False,\n","        num_workers=4,\n","    )\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model = NN(\n","        num_cont_cols=hyperparams[\"num_cont_cols\"],\n","        embed_dim=hyperparams[\"embed_dim\"],\n","        num_layers=hyperparams[\"num_layers\"],\n","        num_heads=hyperparams[\"num_heads\"],\n","        max_seq_len=hyperparams[\"max_seq_len\"],\n","    ).to(device)\n","\n","\n","    loss_function = torch.nn.BCEWithLogitsLoss()\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), hyperparams[\"learning_rate\"])\n","\n","    num_epochs = 80\n","\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_epochs*len(train_loader)//10,\n","        num_training_steps= num_epochs*len(train_loader)\n","    )\n","\n","    val_interval = 1\n","    best_val_loss = 1e6\n","    best_val_score = -1\n","    best_model = model\n","\n","    for epoch in range(num_epochs):\n","        torch.manual_seed(hyperparams[\"seed\"])\n","        model.train()\n","        epoch_loss = 0\n","\n","        for x1s, x2s, x3s, ys in train_loader:\n","            x1s, x2s, x3s, ys = [t.to(device).type(torch.float) for t in [x1s, x2s, x3s, ys]]\n","\n","            # takes one level group at a time, since the sequence length varies\n","\n","            out = model(x1s, \"0-4\")\n","            loss = loss_function(out, ys[:, :3].unsqueeze(2)) *3/18\n","            epoch_loss += loss.item()\n","            loss.backward()\n","\n","            out = model(x2s, \"5-12\")\n","            loss = loss_function(out, ys[:, 3:13].unsqueeze(2)) *10/18\n","            epoch_loss += loss.item()\n","            loss.backward()\n","\n","            g3_mask = x3s.sum(dim=(1, 2)) != 0\n","            out = model(x3s[g3_mask], \"13-22\")\n","            loss = loss_function(out, ys[g3_mask, 13:].unsqueeze(2)) *5/18\n","            epoch_loss += loss.item()\n","            loss.backward()\n","\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","        epoch_loss /= len(train_loader)\n","        writer.add_scalar('Loss/train', epoch_loss, epoch)\n","\n","\n","        if epoch % val_interval == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                val_epoch_loss = 0\n","                targs = []\n","                val_probas = []\n","                for x1s, x2s, x3s, ys in val_loader:\n","                    x1s, x2s, x3s, ys = [t.to(device).type(torch.float) for t in [x1s, x2s, x3s, ys]]\n","\n","                    out = model(x1s, \"0-4\")\n","                    loss = loss_function(out, ys[:, :3].unsqueeze(2)) *3/18\n","                    val_epoch_loss += loss.item()\n","                    probas1 = torch.sigmoid(out)\n","\n","                    out = model(x2s, \"5-12\")\n","                    loss = loss_function(out, ys[:, 3:13].unsqueeze(2)) *10/18\n","                    val_epoch_loss += loss.item()\n","                    probas2 = torch.sigmoid(out)\n","\n","                    out = model(x3s, \"13-22\")\n","                    loss = loss_function(out, ys[:, 13:].unsqueeze(2)) *5/18\n","                    val_epoch_loss += loss.item()\n","                    probas3 = torch.sigmoid(out)\n","\n","                    probas = torch.cat([probas1, probas2, probas3], dim=1)\n","                    val_probas.append(probas.cpu().numpy())\n","                    targs.append(ys.cpu().numpy())\n","\n","                targs = np.concatenate(targs)\n","                val_probas = np.concatenate(val_probas)\n","\n","                best_score, best_threshold = threshold_score(val_probas.flatten(), targs.flatten())\n","\n","                val_epoch_loss /= len(val_loader)\n","                writer.add_scalar('Loss/val', val_epoch_loss, epoch)\n","                writer.add_scalar('Score/val', best_score, epoch)\n","\n","                if val_epoch_loss < best_val_loss:\n","                    best_val_loss = val_epoch_loss\n","                    best_val_score = best_score\n","                    best_model = copy.deepcopy(model)\n","                    best_val_probas = val_probas\n","\n","        print(\"epoch\", epoch, \"best_score\", best_val_score)\n","\n","\n","\n","    writer.flush()\n","    return best_model, best_val_probas\n","\n","\n","def predict(hyperparams, test_x, test_y, seq_lens, model_path):\n","    \"\"\"\n","    Load trained transformer from model_path,\n","    and perform prediction on the data test_x\n","    \"\"\"\n","\n","    val_gen = DataGenerator(test_x, test_y, seq_lens)\n","    val_loader = DataLoader(\n","        val_gen,\n","        batch_size=hyperparams[\"batch_size\"],\n","        shuffle=False,\n","        num_workers=4,\n","    )\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model = NN(\n","        num_cont_cols=hyperparams[\"num_cont_cols\"],\n","        embed_dim=hyperparams[\"embed_dim\"],\n","        num_layers=hyperparams[\"num_layers\"],\n","        num_heads=hyperparams[\"num_heads\"],\n","        max_seq_len=hyperparams[\"max_seq_len\"],\n","    ).to(device)\n","\n","    model.load_state_dict(torch.load(model_path))\n","\n","    model.eval()\n","    with torch.no_grad():\n","        targs = []\n","        val_probas = []\n","        for x1s, x2s, x3s, ys in val_loader:\n","            x1s, x2s, x3s, ys = [t.to(device).type(torch.float) for t in [x1s, x2s, x3s, ys]]\n","\n","            # takes one level group at a time, since the sequence length varies\n","\n","            out = model(x1s, \"0-4\")\n","            probas1 = torch.sigmoid(out)\n","\n","            out = model(x2s, \"5-12\")\n","            probas2 = torch.sigmoid(out)\n","\n","            out = model(x3s, \"13-22\")\n","            probas3 = torch.sigmoid(out)\n","\n","            probas = torch.cat([probas1, probas2, probas3], dim=1)\n","            val_probas.append(probas.cpu().numpy())\n","            targs.append(ys.cpu().numpy())\n","\n","        targs = np.concatenate(targs)\n","        val_probas = np.concatenate(val_probas)\n","\n","        best_score, best_threshold = threshold_score(val_probas.flatten(), targs.flatten())\n","\n","    return model, val_probas\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETSQVL0-7Gaq"},"outputs":[],"source":["%%time\n","\n","# Transfomer training\n","\n","n_splits = 5\n","\n","kf = KFold(n_splits=n_splits)\n","\n","seeds = [0, 1, 2]\n","for s in seeds:\n","\n","    params = {\n","        'learning_rate': 0.001,\n","        \"batch_size\": 512,\n","        \"num_cont_cols\": 5,\n","        \"embed_dim\": 64,\n","        \"max_seq_len\": seq_lens[-1],\n","        \"num_layers\": 1,\n","        \"num_heads\": 8,\n","        'early_stopping_rounds': 15,\n","        \"seed\": s\n","    }\n","\n","    limits = {'0-4': range(1, 4), '5-12': range(4, 14), '13-22': range(14, 19)}\n","\n","    preds = []\n","    targs = []\n","\n","    fold_scores = []\n","\n","    for i, (train_index, test_index) in enumerate(kf.split(val_sids)):\n","\n","        test_users = val_sids[test_index]\n","\n","        # Doing this to validate on the kaggle data while also using the raw data for training\n","        train_x = df_tr.filter(~pl.col(\"session_id\").is_in(test_users))\n","        train_users = train_x[\"session_id\"].unique(maintain_order=True)\n","\n","        # normalize the input\n","        train_x = train_x.join(norms, on=\"point\", how=\"left\")\n","        train_x = train_x.with_columns([\n","            (pl.col(\"et_diff\") - pl.col(\"et_mean\")) / pl.col(\"et_std\"),\n","            (pl.col(\"ix_diff\") - pl.col(\"ix_mean\")) / pl.col(\"ix_std\"),\n","            (pl.col(\"dist_diff\") - pl.col(\"dist_mean\")) / pl.col(\"dist_std\"),\n","            (pl.col(\"room_coor_x\") - pl.col(\"room_coor_x_mean\")) / pl.col(\"room_coor_x_std\"),\n","            (pl.col(\"room_coor_y\") - pl.col(\"room_coor_y_mean\")) / pl.col(\"room_coor_y_std\"),\n","        ]).fill_nan(0)\n","        # replace some outlier values with 0\n","        train_x = train_x.with_columns([pl.when(pl.col(\"ix_diff\").abs() > 7.5).then(0).otherwise(pl.col(\"ix_diff\")).keep_name()])\n","        # map point categoricals to integer index, so transformers can handle it\n","        train_x = train_x.with_columns([pl.col(\"point\").map_dict(point_ixs)])\n","\n","        test_x = df_tr.filter(pl.col(\"session_id\").is_in(test_users))\n","\n","        test_x = test_x.join(norms, on=\"point\", how=\"left\")\n","        test_x = test_x.with_columns([\n","            (pl.col(\"et_diff\") - pl.col(\"et_mean\")) / pl.col(\"et_std\"),\n","            (pl.col(\"ix_diff\") - pl.col(\"ix_mean\")) / pl.col(\"ix_std\"),\n","            (pl.col(\"dist_diff\") - pl.col(\"dist_mean\")) / pl.col(\"dist_std\"),\n","            (pl.col(\"room_coor_x\") - pl.col(\"room_coor_x_mean\")) / pl.col(\"room_coor_x_std\"),\n","            (pl.col(\"room_coor_y\") - pl.col(\"room_coor_y_mean\")) / pl.col(\"room_coor_y_std\"),\n","        ]).fill_nan(0)\n","        test_x = test_x.with_columns([pl.when(pl.col(\"ix_diff\").abs() > 7.5).then(0).otherwise(pl.col(\"ix_diff\")).keep_name()])\n","\n","        test_x = test_x.with_columns([pl.col(\"point\").map_dict(point_ixs)])\n","\n","        train_users = train_x[\"session_id\"].unique(maintain_order=True)\n","        train_y = targets_tr.filter(pl.col(\"session_id\").is_in(train_users))\n","        # to sort labels and data in the same order\n","        train_y = train_y.join(pl.DataFrame(train_users), on=\"session_id\")\n","        train_y = train_y.drop(\"session_id\").to_numpy()\n","\n","        test_users = test_x[\"session_id\"].unique(maintain_order=True)\n","        test_y = targets_tr.filter(pl.col(\"session_id\").is_in(test_users))\n","        test_y = test_y.join(pl.DataFrame(test_users), on=\"session_id\")\n","        test_y = test_y.drop(\"session_id\").to_numpy()\n","\n","        train_x = [[a.filter(pl.col(\"level_group\")==grp).select([\"et_diff\", \"ix_diff\",  \"dist_diff\", \"room_coor_x\", \"room_coor_y\", \"point\"]).to_numpy() for grp in [\"0-4\", \"5-12\", \"13-22\"]] for _, a in train_x.groupby([\"session_id\"], maintain_order=True)]\n","        test_x = [[a.filter(pl.col(\"level_group\")==grp).select([\"et_diff\", \"ix_diff\",  \"dist_diff\", \"room_coor_x\", \"room_coor_y\", \"point\"]).to_numpy() for grp in [\"0-4\", \"5-12\", \"13-22\"]] for _, a in test_x.groupby([\"session_id\"], maintain_order=True)]\n","\n","        model_path = f\"models/torch_models/model_{i}_s{s}\"\n","        if os.path.exists(model_path):\n","            shutil.make_archive(model_path, 'zip', model_path)\n","            model_path += \".zip\"\n","            model, val_probas = predict(params, test_x, test_y, trimmed_seq_lens, model_path)\n","        elif os.path.exists(model_path + \".zip\"):\n","            model, val_probas = predict(params, test_x, test_y, trimmed_seq_lens, model_path + \".zip\")\n","        else:\n","            model, val_probas = train(params, train_x, train_y, test_x, test_y, trimmed_seq_lens)\n","\n","            if SAVE_FOR_SUB:\n","                torch.save(model.state_dict(), model_path)\n","\n","        for j in range(18):\n","            tr_oof.loc[test_users, f\"q{j+1}\"] = val_probas[:, j, :]\n","\n","        pred = val_probas.flatten()\n","        targ = test_y.flatten()\n","        best_score, best_threshold = threshold_score(pred, targ)\n","\n","        preds.append(pred)\n","        targs.append(targ)\n","        fold_scores.append(best_score)\n","\n","        print(f\"fold_score_{i}:\", best_score)\n","\n","\n","    preds = np.concatenate(preds)\n","    targs = np.concatenate(targs)\n","\n","    best_score, best_threshold = threshold_score(preds, targs)\n","\n","    tr_oof.to_csv(f\"models/torch_models/tr_oof_s{s}.csv\")\n","\n","    print(\"-\"*10)\n","    print(\"cv_score:\", best_score)\n","    print(\"best_threshold:\", best_threshold)\n","\n","    for i, score in enumerate(fold_scores):\n","        print(f\"fold_score_{i}:\", score)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCGfDPk5y2VD"},"outputs":[],"source":["# evaluate transformer seeds average\n","\n","targs = targets_xgb.set_index('session').loc[val_sids][\"correct\"].values\n","\n","seeds_preds = 0\n","for s in seeds:\n","    preds = pd.read_csv(f\"models/torch_models/tr_oof_s{s}.csv\").set_index(\"session_id\")\n","    seeds_preds += preds.values.flatten() / len(seeds)\n","\n","threshold_score(seeds_preds, targs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJe52SeAxuq-"},"outputs":[],"source":["%%time\n","\n","# Catboost training\n","\n","seeds = [0, 1, 2]\n","for s in seeds:\n","\n","    n_splits = 5\n","\n","    kf = KFold(n_splits=n_splits)\n","\n","    params = {\n","        'loss_function':'Logloss',\n","        'learning_rate': 0.05,\n","        'depth': 8,\n","        'iterations': 1000,\n","        'early_stopping_rounds': 50,\n","        'subsample': 0.8,\n","        'colsample_bylevel': 0.4,\n","        'metric_period': 1,\n","        \"l2_leaf_reg\": 8,\n","        \"random_seed\": s,\n","        'verbose': 0,\n","        'use_best_model': True,\n","    }\n","\n","    limits = {'0-4': range(1, 4), '5-12': range(4, 14), '13-22': range(14, 19)}\n","\n","    fold_preds = []\n","    fold_targs = []\n","\n","    for i, (train_index, test_index) in enumerate(kf.split(val_sids)):\n","\n","        grp_preds = []\n","        grp_targs = []\n","\n","        for df, features, grp in [\n","            (df1_xgb, features1, \"0-4\"),\n","            (df2_xgb, features2, \"5-12\"),\n","            (df3_xgb, features3, \"13-22\")\n","        ]:\n","\n","            test_users = val_sids[test_index]\n","            test_x = df[features].loc[test_users]\n","\n","            # Doing this to validate on the kaggle data while using the raw data for training\n","            train_x = df[features].loc[~df.index.isin(test_users)]\n","            train_users = train_x.index.values\n","\n","            train_y = targets_xgb.loc[targets_xgb[\"q\"].isin(limits[grp])].set_index('session').loc[train_users]\n","            train_y = train_y.sort_values(by=[\"session\", \"q\"])\n","            train_x = pd.concat([train_x]*len(limits[grp])).sort_values(by=[\"session_id\"])\n","            train_x[\"q\"] = train_y[\"q\"].values\n","            train_y = train_y['correct'].values\n","\n","            test_y = targets_xgb.loc[targets_xgb[\"q\"].isin(limits[grp])].set_index('session').loc[test_users]\n","            test_y = test_y.sort_values(by=[\"session\", \"q\"])\n","            test_x = pd.concat([test_x]*len(limits[grp])).sort_values(by=[\"session_id\"])\n","            test_x[\"q\"] = test_y[\"q\"].values\n","            test_y = test_y['correct'].values\n","\n","            clf = CatBoostClassifier(**params)\n","\n","            train_pool = Pool(train_x, train_y)\n","            valid_pool = Pool(test_x, test_y)\n","\n","            model_path = f\"models/cat_models/model_{i}_grp_{grp}_s{s}.cbm\"\n","            if os.path.exists(model_path):\n","                clf.load_model(model_path)\n","            else:\n","                clf = clf.fit(train_pool, eval_set=valid_pool)\n","\n","            val_probas = clf.predict_proba(valid_pool)[:, 1]\n","\n","            grp_preds.append(val_probas)\n","            grp_targs.append(test_y)\n","\n","            if SAVE_FOR_SUB:\n","                clf.save_model(model_path)\n","\n","            for j, q in enumerate(limits[grp]):\n","                xgb_oof.loc[test_x.index.unique(), f\"q{q}\"] = val_probas[j::len(limits[grp])]\n","\n","\n","        preds = np.concatenate(grp_preds)\n","        targs = np.concatenate(grp_targs)\n","\n","        best_score, best_threshold = threshold_score(preds, targs)\n","\n","        fold_preds.append(preds)\n","        fold_targs.append(targs)\n","\n","        print(f\"fold_score_{i}:\", best_score)\n","\n","    preds = np.concatenate(fold_preds)\n","    targs = np.concatenate(fold_targs)\n","\n","    best_score, best_threshold = threshold_score(preds, targs)\n","\n","    xgb_oof.to_csv(f\"models/cat_models/cat_oof_s{s}.csv\")\n","\n","    print(\"-\"*10)\n","    print(\"cv_score:\", best_score)\n","    print(\"best_threshold:\", best_threshold)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EztLggKsxutn"},"outputs":[],"source":["# evaluate catboost seeds average\n","\n","targs = targets_xgb.set_index('session').loc[val_sids][\"correct\"].values\n","\n","seeds_preds = 0\n","for s in seeds:\n","    preds = pd.read_csv(f\"models/cat_models/cat_oof_s{s}.csv\").set_index(\"session_id\")\n","    seeds_preds += preds.values.flatten() / len(seeds)\n","\n","threshold_score(seeds_preds, targs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rz22S6YCavns"},"outputs":[],"source":["%%time\n","\n","# train and evaluate ensemble with linear regression\n","\n","n_splits = 5\n","kf = KFold(n_splits=n_splits)\n","\n","limits = {'0-4': range(1, 4), '5-12': range(4, 14), '13-22': range(14, 19)}\n","fold_preds = []\n","fold_targs = []\n","seeds = [0, 1, 2]\n","\n","for q in range(1, 19):\n","\n","    if 1 <= q <= 3:\n","        # number of questions used as feature\n","        qs = 3\n","    elif 4 <= q <= 13:\n","        qs = 13\n","    else:\n","        qs = 18\n","\n","    preds = []\n","    targs = []\n","\n","    for i, (train_index, test_index) in enumerate(kf.split(val_sids)):\n","\n","        train_users = val_sids[train_index]\n","        test_users = val_sids[test_index]\n","\n","        train_y = targets_xgb.loc[targets_xgb[\"q\"] == q].set_index('session').loc[train_users]\n","\n","        # calculate seeds average\n","        train_x = 0\n","        for s in seeds:\n","            preds_xgb = pd.read_csv(f\"models/xgb_models/xgb_oof_s{s}.csv\").set_index(\"session_id\")\n","            preds_xgb = preds_xgb.loc[train_users, [f\"q{q+1}\" for q in range(qs)]]\n","            preds_cat = pd.read_csv(f\"models/cat_models/cat_oof_s{s}.csv\").set_index(\"session_id\")\n","            preds_cat = preds_cat.loc[train_users, [f\"q{q+1}\" for q in range(qs)]]\n","            preds_tr = pd.read_csv(f\"models/torch_models/tr_oof_s{s}.csv\").set_index(\"session_id\")\n","            preds_tr = preds_tr.loc[train_users, [f\"q{q+1}\" for q in range(qs)]]\n","            preds_ens = pd.concat([preds_xgb, preds_cat, preds_tr], axis=1)\n","            train_x += preds_ens.values / len(seeds)\n","\n","        test_y = targets_xgb.loc[targets_xgb[\"q\"] == q].set_index('session').loc[test_users]\n","\n","        test_x = 0\n","        for s in seeds:\n","            preds_xgb = pd.read_csv(f\"models/xgb_models/xgb_oof_s{s}.csv\").set_index(\"session_id\")\n","            preds_xgb = preds_xgb.loc[test_users, [f\"q{q+1}\" for q in range(qs)]]\n","            preds_cat = pd.read_csv(f\"models/cat_models/cat_oof_s{s}.csv\").set_index(\"session_id\")\n","            preds_cat = preds_cat.loc[test_users, [f\"q{q+1}\" for q in range(qs)]]\n","            preds_tr = pd.read_csv(f\"models/torch_models/tr_oof_s{s}.csv\").set_index(\"session_id\")\n","            preds_tr = preds_tr.loc[test_users, [f\"q{q+1}\" for q in range(qs)]]\n","            preds_ens = pd.concat([preds_xgb, preds_cat, preds_tr], axis=1)\n","            test_x += preds_ens.values / len(seeds)\n","\n","        clf = LinearRegression()\n","        clf.fit(train_x, train_y['correct'].values)\n","\n","        pickle.dump(clf, open(f'models/meta_models/lin_reg_{i}_q{q}.sav', 'wb'))\n","\n","        val_probas = clf.predict(test_x)\n","\n","        preds.append(val_probas)\n","        targs.append(test_y['correct'].values)\n","\n","    preds = np.concatenate(preds)\n","    targs = np.concatenate(targs)\n","\n","    fold_preds.append(preds)\n","    fold_targs.append(targs)\n","\n","preds = np.concatenate(fold_preds)\n","targs = np.concatenate(fold_targs)\n","\n","best_score, best_threshold = threshold_score(preds, targs)\n","\n","print(\"-\"*10)\n","print(\"cv_score:\", best_score)\n","print(\"best_threshold:\", best_threshold)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","mount_file_id":"1vRklsyb_1ggrwAxae0YaTWk7SBKYtCb6","authorship_tag":"ABX9TyO7KzhwR2xHBSwBJDUW9NOq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}